{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import math\n",
    "from typing import Union, Optional, List, Iterable, Dict, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from Analysis.hyperparameter_tuning import mean_relative_error, mean_absolute_error\n",
    "from Analysis.regression_ATE import get_one_domain_out_cv, create_regression_dataset\n",
    "from Analysis.analysis_utils import feature_cols, get_feature_sets, confidence_intervals\n",
    "from Baselines.correlate_metrics import draw_regression, smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prevent text getting cut off in saved figures\n",
    "mpl.rcParams['savefig.bbox'] = 'tight'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap = False\n",
    "n_grams = 'UNI'\n",
    "n_concepts = 6\n",
    "use_acc = True\n",
    "sort_by_ate = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, y = create_regression_dataset(shap, n_grams, n_concepts, rows_sorted=sort_by_ate, use_acc=use_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label_column = 'performance_degradation' + ('_acc' if use_acc else '')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X['source_acc'] - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "Mainly scaling the data. No need to remove duplicates, normalize each row separately.\n",
    "Check for outliers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling the input features to standard uniform distribution."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_scaler = StandardScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TABLES_PATH = Path(r'E:\\OneDrive - Technion\\Technion\\Graduate\\Thesis tables')\n",
    "IMAGES_PATH = Path(r'E:\\OneDrive - Technion\\Technion\\Graduate\\Thesis images')\n",
    "TABLES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_to_scale = feature_cols(n_concepts)\n",
    "features_to_scale.remove('source_f1')\n",
    "features_to_scale.remove('source_acc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.loc[:, features_to_scale] = feature_scaler.fit_transform(X[features_to_scale])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling columns having to do with percentages to percentages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_columns = ['source_acc']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X.loc[:, acc_columns] = X[acc_columns] * 100\n",
    "if use_acc:\n",
    "    y *= 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exploration\n",
    "Visualisations, correlations, distribution by source and target domains etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_df = X.copy()\n",
    "joint_df.loc[:, label_column] = y\n",
    "joint_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_df[label_column].mean(), joint_df[label_column].std()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joint_df[label_column].plot(kind='kde')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.scatter(x='source_acc' if use_acc else 'source_f1', y=label_column, data=joint_df)\n",
    "plt.xlabel('source accuracy')\n",
    "plt.ylabel('performance drop')\n",
    "plt.savefig(str(IMAGES_PATH / 'source_acc_correlation.eps'), format='eps')\n",
    "plt.savefig(str(IMAGES_PATH / 'source_acc_correlation.pdf'), format='pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_source_data = joint_df.groupby('source').aggregate({label_column: [list, 'mean', 'std']})\n",
    "per_source_data = per_source_data.drop(columns=[('performance_degradation_acc', 'list')]).droplevel(level=0, axis=1).sort_values('mean', ascending=False)\n",
    "# per_source_data.plot(kind='bar', figsize=(136, 64))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_source_data = per_source_data.reset_index(drop=False)\n",
    "per_source_data['source'] = per_source_data['source'].apply(lambda domain: \" \".join(domain.split('_')[1:]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_source_data.to_latex(str(TABLES_PATH / 'source_domain_stats.tex'), float_format=\"${:0.4f}$\".format, index=False, escape=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_target_data = joint_df.groupby('target').aggregate({label_column: [list, 'mean', 'std']})\n",
    "# per_target_data.plot(kind='bar', figsize=(136, 64))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_target_data = per_target_data.drop(columns=[('performance_degradation_acc', 'list')]).droplevel(level=0, axis=1).sort_values('mean', ascending=False)\n",
    "per_target_data = per_target_data.reset_index(drop=False)\n",
    "per_target_data['target'] = per_target_data['target'].apply(lambda domain: \" \".join(domain.split('_')[1:]))\n",
    "per_target_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_target_data.to_latex(str(TABLES_PATH / 'target_domain_stats.tex'), float_format=\"${:0.4f}$\".format, index=False, escape=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_domain_data = per_source_data.rename(columns={'source': 'Domain'}).set_index('Domain')\n",
    "per_domain_data = pd.DataFrame(data=per_domain_data.values, index=per_domain_data.index, columns=[['as source'] * len(per_domain_data.columns), per_domain_data.columns])\n",
    "per_domain_data[[('as target', 'mean'), ('as target', 'std')]] = per_target_data.set_index('target')\n",
    "per_domain_data = per_domain_data.sort_values(('as source', 'mean'), ascending=False)\n",
    "per_domain_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "per_domain_data.to_latex(str(TABLES_PATH / 'per_domain_stats.tex'), float_format=\"${:0.4f}$\".format, index=True, escape=False, multicolumn=True, multicolumn_format='c', column_format='lcccc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic models\n",
    "Try different models with default parameters, using basic feature configurations: all features, all non-ate and non-SHAP features, all but SHAP, all but ATE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_funcs = [\n",
    "    r2_score,\n",
    "    mean_relative_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error\n",
    "]\n",
    "metrics = [\n",
    "    make_scorer(r2_score),\n",
    "    make_scorer(mean_relative_error, greater_is_better=False),\n",
    "    make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "]\n",
    "metric_names = ['r2', 'relative_error', 'RMSE', 'absolute_error']\n",
    "error_metrics = ['relative_error', 'RMSE', 'absolute_error']\n",
    "\n",
    "confidence_interval_cols = []\n",
    "for metric_name in metric_names:\n",
    "    confidence_interval_cols.extend([f'{metric_name}_low', f'{metric_name}_high'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "models = [LinearRegression(), ElasticNet(), GradientBoostingRegressor()]\n",
    "models = {type(model).__name__: model for model in models}\n",
    "domains_cv = get_one_domain_out_cv(X)\n",
    "columns = ['model_name', 'feature_set'] + metric_names + confidence_interval_cols\n",
    "scores_df = pd.DataFrame(columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "default_metrics_df = pd.DataFrame(columns=['model_name', 'feature_set'] + metric_names)\n",
    "for model_name, model in models.items():\n",
    "    cv_results = cross_validate(model, X[feature_cols(n_concepts)], y, cv=domains_cv, scoring=dict(zip(metric_names, metrics)), error_score='raise')\n",
    "    default_metrics_df = default_metrics_df.append(pd.DataFrame([[model_name, 'all_features'] + [cv_results[f'test_{name}'].mean() for name in metric_names]], columns=['model_name', 'feature_set'] + metric_names))\n",
    "default_metrics_df[error_metrics] *= -1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "default_metrics_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning\n",
    "Tuning hyperparameters separately for every model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[LinearRegression(), Lasso(), Ridge(), ElasticNet(), RandomForestRegressor(), GradientBoostingRegressor()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_models_param_grid(num_features):\n",
    "    model_param_grids = {\n",
    "        'LinearRegression': {},\n",
    "        'Lasso': {\n",
    "            'alpha': np.e ** np.linspace(-3, 5, 10),\n",
    "            'warm_start': [True, False]\n",
    "        },\n",
    "        'Ridge': {\n",
    "            'alpha': np.e ** np.linspace(-3, 5, 10)\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'alpha': np.e ** np.linspace(-3, 5, 10),\n",
    "            'l1_ratio': np.linspace(0, 1, 10),\n",
    "            'warm_start': [True, False],\n",
    "            'max_iter': [int(1e4)]\n",
    "        },\n",
    "        'RandomForestRegressor': {\n",
    "            'n_estimators': (2 ** np.linspace(1, 7) - 1).astype(int),\n",
    "            'max_depth': list(range(1, 7)),\n",
    "            'bootstrap': [True, False],\n",
    "            'max_features': ['auto', 1, np.sqrt(num_features), num_features]\n",
    "        },\n",
    "        'GradientBoostingRegressor': {\n",
    "            'n_estimators': (2 ** np.linspace(1, 7) - 1).astype(int),\n",
    "            'max_depth': list(range(1, 7)),\n",
    "            'max_features': ['auto', 1, int(np.sqrt(num_features)), (num_features)]\n",
    "        }\n",
    "    }\n",
    "    return model_param_grids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_tuned_model_stats(features, metrics: Dict[str, Any], refit: str = False) -> Tuple[pd.DataFrame, Dict]:\n",
    "    grids = get_models_param_grid(len(features))\n",
    "    agg_cv_results = {}\n",
    "    confidence_interval_cols = []\n",
    "    for metric_name in metrics.keys():\n",
    "        confidence_interval_cols.extend([f'{metric_name}_low', f'{metric_name}_high'])\n",
    "    agg_cv_results_df = pd.DataFrame(columns=['model_name'] + list(metrics.keys()) + confidence_interval_cols)\n",
    "    agg_cv_results_df.loc[:, 'model_name'] = list(models.keys())\n",
    "    for model_name, model in models.items():\n",
    "        gscv = GridSearchCV(model, grids[model_name], scoring=metrics, n_jobs=10, cv=domains_cv,\n",
    "                            refit=refit, verbose=0)\n",
    "        gscv.fit(X=X[features], y=y)\n",
    "        results_df = pd.DataFrame.from_dict(gscv.cv_results_, orient='columns')\n",
    "        for metric_name in metrics.keys():\n",
    "            results_df[f'mean_{metric_name}'] = results_df[[f'split{i}_test_{metric_name}' for i in range(len(domains_cv))]].mean(axis=1)\n",
    "        best_indices = {\n",
    "            metric_name: results_df[f'mean_{metric_name}'].idxmax()\n",
    "            for metric_name in metrics.keys()\n",
    "        }\n",
    "        scores = {\n",
    "            metric_name: np.array([gscv.cv_results_[f'split{i}_test_{metric_name}'][best_indices[metric_name]] for i in range(len(domains_cv))])\n",
    "            for metric_name in metrics\n",
    "        }\n",
    "        best_scores = {\n",
    "            metric_name: metric_scores.mean()\n",
    "            for metric_name, metric_scores in scores.items()\n",
    "        }\n",
    "        intervals = {}\n",
    "        for metric_name, metric_scores in scores.items():\n",
    "            if metric_name in error_metrics:\n",
    "                metric_scores *= -1\n",
    "            low, high = confidence_intervals(metric_scores)\n",
    "            intervals[f'{metric_name}_low'] = low\n",
    "            intervals[f'{metric_name}_high'] = high\n",
    "        for metric_name, metric_value in best_scores.items():\n",
    "            if metric_name in error_metrics:\n",
    "                best_scores[metric_name] = -metric_value\n",
    "        if 'RMSE' in best_scores:\n",
    "            best_scores['RMSE'] = math.sqrt(best_scores['RMSE'])\n",
    "        for metric in metrics.keys():\n",
    "            agg_cv_results_df.loc[agg_cv_results_df['model_name'] == model_name, metric] = best_scores[metric]\n",
    "            agg_cv_results_df.loc[agg_cv_results_df['model_name'] == model_name, f'{metric}_low'] = intervals[f'{metric}_low']\n",
    "            agg_cv_results_df.loc[agg_cv_results_df['model_name'] == model_name, f'{metric}_high'] = intervals[f'{metric}_high']\n",
    "        agg_cv_results[model_name] = {\n",
    "            'params': gscv.best_params_,\n",
    "            'estimator': gscv.best_estimator_,\n",
    "            'scores': best_scores\n",
    "        }\n",
    "    return agg_cv_results_df, agg_cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cv_results = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with tqdm(get_feature_sets(num_concepts=n_concepts).items(), desc='Tuning regressors') as pbar:\n",
    "    for feature_names, features in pbar:\n",
    "        pbar.set_postfix(features=feature_names)\n",
    "        best_metrics, local_cv_results = get_tuned_model_stats(features, dict(zip(metric_names, metrics)), refit=f'absolute_error')\n",
    "        best_metrics.loc[:, 'feature_set'] = feature_names\n",
    "        scores_df = scores_df.append(best_metrics, ignore_index=True)\n",
    "        cv_results[feature_names] = local_cv_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df[scores_df['model_name'] == 'GradientBoostingRegressor'][['feature_set', 'relative_error', 'absolute_error', 'RMSE']].to_latex(\n",
    "    str(TABLES_PATH / f'gbr_{n_grams}_feature_sets.tex'),\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    float_format=\"${:0.4f}$\".format,\n",
    ")\n",
    "\n",
    "scores_df[scores_df['model_name'] == 'ElasticNet'][['feature_set', 'relative_error', 'absolute_error', 'RMSE']].to_latex(\n",
    "    str(TABLES_PATH / f'en_{n_grams}_feature_sets.tex'),\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    float_format=\"${:0.4f}$\".format,\n",
    ")\n",
    "\n",
    "scores_df[scores_df['model_name'] == 'ElasticNet'][['feature_set', 'absolute_error', 'absolute_error_low', 'absolute_error_high']].to_latex(\n",
    "    str(TABLES_PATH / f'en_{n_grams}_mae_with_ci.tex'),\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    float_format=\"${:0.4f}$\".format\n",
    ")\n",
    "\n",
    "scores_df[scores_df['model_name'] == 'GradientBoostingRegressor'][['feature_set', 'absolute_error', 'absolute_error_low', 'absolute_error_high']].to_latex(\n",
    "    str(TABLES_PATH / f'gbr_{n_grams}_mae_with_ci.tex'),\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    float_format=\"${:0.4f}$\".format\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df[scores_df['model_name'] == 'GradientBoostingRegressor'][['feature_set', 'relative_error', 'absolute_error', 'RMSE']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df.to_excel(r'E:\\OneDrive - Technion\\Technion\\Graduate\\Research\\analysis tables\\kmeans_metrics_per_feature_set.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric_name in ['absolute_error', 'relative_error', 'RMSE']:\n",
    "    scores_df[scores_df['model_name'] == 'GradientBoostingRegressor'][['feature_set', metric_name]]\\\n",
    "        .plot(kind='bar', x='feature_set')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(str(IMAGES_PATH / f'generalization_results_{metric_name}_{n_grams}_gbr.eps'), format='eps')\n",
    "\n",
    "for metric_name in ['absolute_error', 'relative_error', 'RMSE']:\n",
    "    scores_df[scores_df['model_name'] == 'ElasticNet'][['feature_set', metric_name]]\\\n",
    "        .plot(kind='bar', x='feature_set')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(str(IMAGES_PATH / f'generalization_results_{metric_name}_{n_grams}_en.eps'), format='eps')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(scores_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "desired_metrics = ['relative_error', 'absolute_error']\n",
    "per_model_data = pd.concat([scores_df[scores_df['model_name'] == model_name].set_index('feature_set')[desired_metrics] for model_name in scores_df['model_name'].unique()], keys=scores_df['model_name'].unique().tolist(), axis=1)\n",
    "per_model_data.to_latex(str(TABLES_PATH / f'all_model_comparison_{n_grams}.tex'), index=True, multicolumn=True, float_format=\"${:0.4g}$\".format, multicolumn_format='c', escape=False, column_format='l|cc|cc|cc')\n",
    "per_model_data[['ElasticNet', 'GradientBoostingRegressor']].to_latex(str(TABLES_PATH / f'part_model_comparison_{n_grams}.tex'), index=True, multicolumn=True, float_format=\"${:0.4g}$\".format, multicolumn_format='c', escape=False, column_format='l|cc|cc')\n",
    "per_model_data[['LinearRegression', 'ElasticNet']].to_latex(str(TABLES_PATH / f'linear_model_comparison_{n_grams}.tex'), index=True, multicolumn=True, float_format=\"${:0.4g}$\".format, multicolumn_format='c', escape=False, column_format='l|cc|cc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Analyzing the tuned models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyzing the feature importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_weights_equivalent(results_dict: Dict[str, Any], feature_set: str, estimator_class: str) -> Iterable[float]:\n",
    "    estimator = results_dict[feature_set][estimator_class]['estimator']\n",
    "    if type(estimator) in (LinearRegression, Lasso, Ridge, ElasticNet):\n",
    "        return estimator.coef_\n",
    "    else:\n",
    "        assert (isinstance(estimator, GradientBoostingRegressor) or isinstance(estimator, RandomForestRegressor)), f'expected tree ensemble type, got {type(estimator)}'\n",
    "        return estimator.feature_importances_\n",
    "\n",
    "def features_by_importance(results_dict: Dict[str, Any], num_concepts=n_concepts) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for (feature_set_name, feature_set), model_name in product(get_feature_sets(num_concepts).items(), models.keys()):\n",
    "        results.append([\n",
    "            feature_set_name,\n",
    "            model_name,\n",
    "            sorted(zip(feature_set, get_weights_equivalent(results_dict, feature_set_name, model_name)), key=lambda x: -x[1])\n",
    "        ])\n",
    "    feature_importance_df = pd.DataFrame.from_records(results, columns=['feature_set', 'model', 'feature_importances'])\n",
    "    feature_importance_df = feature_importance_df.explode('feature_importances', ignore_index=True)\n",
    "    feature_importance_df.loc[:, ['feature_name', 'feature_importance']] = feature_importance_df['feature_importances'].tolist()\n",
    "    return feature_importance_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outlier_coefficient = 10\n",
    "feature_importances_df = features_by_importance(cv_results)\n",
    "feature_importances_df = feature_importances_df[feature_importances_df['feature_importance'].abs() > 0]\n",
    "# Removing lines with extreme feature importances (over 6 times the STD above or below the mean)\n",
    "# feature_importances_df = feature_importances_df[(feature_importances_df['feature_importance'] - feature_importances_df['feature_importance'].mean()).abs() <= feature_importances_df['feature_importance'].std() * outlier_coefficient]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_importances_df[(~feature_importances_df['feature_set'].isin(['shap', 'shap + baseline', 'shap + baseline + concept DF'])) & (feature_importances_df['model'] == 'LinearRegression')].groupby('feature_name').mean().sort_values(by='feature_importance', ascending=False)[:15].plot(kind='bar')\n",
    "plt.savefig(str(IMAGES_PATH / f'{n_grams}_lr_no_shap_feature_importance.eps'), format='eps')\n",
    "feature_importances_df[(~feature_importances_df['feature_set'].isin(['shap', 'shap + baseline', 'shap + baseline + concept DF'])) & (feature_importances_df['model'] == 'ElasticNet')].groupby('feature_name').mean().sort_values(by='feature_importance', ascending=False)[:15].plot(kind='bar')\n",
    "plt.savefig(str(IMAGES_PATH / f'{n_grams}_en_no_shap_feature_importance.eps'), format='eps')\n",
    "feature_importances_df[(~feature_importances_df['feature_set'].isin(['shap', 'shap + baseline', 'shap + baseline + concept DF'])) & (feature_importances_df['model'] == 'GradientBoostingRegressor')].groupby('feature_name').mean().sort_values(by='feature_importance', ascending=False)[:15].plot(kind='bar')\n",
    "plt.savefig(str(IMAGES_PATH / f'{n_grams}_gbr_no_shap_feature_importance.eps'), format='eps')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyzing error distribution for different feature sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_to_analyze = 'GradientBoostingRegressor'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_error_distribution(X, y, feature_set: str, results_dict: Dict, model_to_analyze: str, metrics: Dict, error_type: str = 'absolute_error'):\n",
    "    loocv = get_one_domain_out_cv(X)\n",
    "    raw_errors = {metric: [] for metric in metrics}\n",
    "    best_model_stats = results_dict[feature_set][model_to_analyze]\n",
    "    model_class = type(best_model_stats['estimator'])\n",
    "    model_params = best_model_stats['params']\n",
    "    features = get_feature_sets(n_concepts)[feature_set]\n",
    "    for train_idx, test_idx in loocv:\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X.loc[X.index.isin(train_idx), features], y.iloc[train_idx])\n",
    "        y_pred = model.predict(X.loc[X.index.isin(test_idx), features])\n",
    "        for metric_name, scorer in metrics.items():\n",
    "            raw_errors[metric_name].extend([scorer(pd.Series([sample_true]), pd.Series([sample_pred])) for sample_true, sample_pred in zip(y.iloc[test_idx], y_pred)])\n",
    "    errors_df = pd.DataFrame.from_dict(raw_errors)\n",
    "    errors_df[error_type].plot(kind='kde', title=feature_set)\n",
    "    return errors_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = display_error_distribution(X, y, 'ates', cv_results, model_to_analyze, dict(zip(metric_names[1:], metric_funcs[1:])), error_type='absolute_error')\n",
    "plt.figure()\n",
    "_ = display_error_distribution(X, y, 'shap', cv_results, model_to_analyze, dict(zip(metric_names[1:], metric_funcs[1:])), error_type='absolute_error')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Displaying regression plot for the learned models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def draw_regression_curve(feature_set: str, num_concepts: int = n_concepts, confidence: float = 0.95):\n",
    "    features = get_feature_sets(num_concepts)[feature_set]\n",
    "    prediction_intervals = defaultdict(list)\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    ax = plt.gca()\n",
    "    domains_cv = get_one_domain_out_cv(X, add_domain_name=True)\n",
    "    for x_train_idx, x_test_idx, domain_name in tqdm(domains_cv):\n",
    "        x_train = X[X.index.isin(x_train_idx)][features]\n",
    "        x_test = X[X.index.isin(x_test_idx) & (X['source'] != domain_name)][features]\n",
    "        y_train = y[y.index.isin(x_train_idx)]\n",
    "        domain_y_true = y[y.index.isin(x_test_idx) & (X['source'] != domain_name)]\n",
    "        estimator_template = cv_results[feature_set]['GradientBoostingRegressor']['estimator']\n",
    "        estimator_params = estimator_template.get_params()\n",
    "\n",
    "        for key in ['loss', 'alpha']:\n",
    "            try:\n",
    "                del estimator_params[key]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        estimators = {\n",
    "            'low': type(estimator_template)(**estimator_params, loss='quantile', alpha=1 - confidence),\n",
    "            'median': type(estimator_template)(**estimator_params, loss='quantile', alpha=0.5),\n",
    "            'high': type(estimator_template)(**estimator_params, loss='quantile', alpha=confidence)\n",
    "        }\n",
    "        estimators = {\n",
    "            key: value.fit(x_train, y_train)\n",
    "            for key, value in estimators.items()\n",
    "        }\n",
    "        y_preds = {\n",
    "            key: value.predict(x_test)\n",
    "            for key, value in estimators.items()\n",
    "        }\n",
    "        for key in y_preds:\n",
    "            prediction_intervals[key].extend(y_preds[key])\n",
    "        prediction_intervals['source_acc'].extend(x_test['source_acc'].tolist())\n",
    "        ax = draw_regression(x_test['source_acc'] - y_preds['median'], x_test['source_acc'] - domain_y_true, x_label='y_pred', y_label='y_true', label=domain_name)\n",
    "\n",
    "    prediction_intervals = pd.DataFrame.from_dict(prediction_intervals, orient='columns')\n",
    "    prediction_intervals.loc[:, 'x_axis'] = prediction_intervals['source_acc'] - prediction_intervals['median']\n",
    "    prediction_intervals = prediction_intervals.drop_duplicates(subset=['x_axis'])\n",
    "    prediction_intervals = prediction_intervals.sort_values('x_axis')\n",
    "    print(len(prediction_intervals.index))\n",
    "    ax.plot(\n",
    "        prediction_intervals['x_axis'], smooth(prediction_intervals['source_acc'] - prediction_intervals['low'], 0.5), 'k:',\n",
    "        prediction_intervals['x_axis'], smooth(prediction_intervals['source_acc'] - prediction_intervals['high'], 0.5), 'k:',\n",
    "    )\n",
    "    plt.savefig(str(IMAGES_PATH / f'{n_grams}_{feature_set}_regression_match_plot.png'), format='png')\n",
    "    plt.savefig(str(IMAGES_PATH / f'{n_grams}_{feature_set}_regression_match_plot.eps'), format='eps')\n",
    "    return plt.gca()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "draw_regression_curve('all - performance', num_concepts=n_concepts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyzing the fit capabilities of the models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model_fit_metrics(X: pd.DataFrame, y: pd.Series, metrics: Dict[str, Any]) -> pd.DataFrame:\n",
    "    fit_metrics = []\n",
    "    for feature_set, features in get_feature_sets(num_concepts=n_concepts).items():\n",
    "        feature_set_estimators = cv_results[feature_set]\n",
    "        for estimator_class in feature_set_estimators.keys():\n",
    "            estimator = feature_set_estimators[estimator_class]['estimator']\n",
    "            scores = {\n",
    "                metric_name: metric(estimator, X[features], y)\n",
    "                for metric_name, metric in metrics.items()\n",
    "            }\n",
    "            fit_metrics.append(dict(feature_set=feature_set, model_class=estimator_class, **scores))\n",
    "    results_df = pd.DataFrame.from_records(fit_metrics)\n",
    "    results_df['RMSE'] = -(np.sqrt((-results_df['RMSE'])))\n",
    "    results_df[error_metrics] *= -1\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fit_results_df = get_model_fit_metrics(X, y, dict(zip(metric_names, metrics)))\n",
    "fit_results_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric_name in ['relative_error', 'RMSE', 'absolute_error']:\n",
    "    fit_results_df[fit_results_df['model_class'] == 'GradientBoostingRegressor'][['feature_set', metric_name]]\\\n",
    "        .plot(kind='bar', x='feature_set')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(str(IMAGES_PATH / f'fit_results_{metric_name}_{n_grams}_gbr.eps'), format='eps')\n",
    "    plt.show()\n",
    "\n",
    "for metric_name in ['relative_error', 'RMSE', 'absolute_error']:\n",
    "    fit_results_df[fit_results_df['model_class'] == 'ElasticNet'][['feature_set', metric_name]]\\\n",
    "        .plot(kind='bar', x='feature_set')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(str(IMAGES_PATH / f'fit_results_{metric_name}_{n_grams}_en.eps'), format='eps')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for metric_name in ['relative_error', 'RMSE', 'absolute_error']:\n",
    "    fit_results_df[fit_results_df['model_class'] == 'LinearRegression'][['feature_set', metric_name]]\\\n",
    "        .plot(kind='bar', x='feature_set')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.savefig(str(IMAGES_PATH / f'fit_results_{metric_name}_{n_grams}_lr.eps'), format='eps')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The effect of #_concepts\n",
    "Testing the effect of the amount of concepts fed to the estimator on the estimator performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_concept_num_cv_results = {}\n",
    "specialized_metric_names = [f'best_{metric}' for metric in metric_names]\n",
    "per_concept_num_scores_df = pd.DataFrame(columns=['model_name', 'feature_set', 'num_concepts'] + specialized_metric_names)\n",
    "for local_n_concepts in tqdm(range(0, n_concepts + 1), desc='n_concepts', leave=False):\n",
    "    per_concept_num_cv_results[local_n_concepts] = {}\n",
    "    for feature_names, features in get_feature_sets(local_n_concepts).items():\n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "        best_metrics, local_cv_results = get_tuned_model_stats(features, dict(zip(specialized_metric_names, metrics)), refit=f'best_absolute_error')\n",
    "        best_metrics.loc[:, 'feature_set'] = feature_names\n",
    "        best_metrics.loc[:, 'num_concepts'] = local_n_concepts\n",
    "        per_concept_num_scores_df = per_concept_num_scores_df.append(best_metrics, ignore_index=True)\n",
    "        per_concept_num_cv_results[local_n_concepts][feature_names] = local_cv_results\n",
    "per_concept_num_scores_df.loc[:, 'best_RMSE'] = -1 * np.sqrt(-1 * per_concept_num_scores_df['best_RMSE'].astype(float).values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_concept_num_scores_df.loc[:, 'best_RMSE'] = per_concept_num_scores_df.apply(lambda row: per_concept_num_cv_results[row['num_concepts']][row['feature_set']][row['model_name']]['scores']['best_RMSE'], axis=1)\n",
    "per_concept_num_scores_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_concept_num_scores_df.loc[:, 'best_RMSE'] = -1 * (np.sqrt(-1 * per_concept_num_scores_df['best_RMSE'].astype(float).values))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "per_concept_num_scores_df[['num_concepts', 'best_RMSE', 'best_absolute_error', 'best_relative_error']].groupby('num_concepts').max().plot(figsize=(16, 12), marker='o')\n",
    "plt.savefig(str(IMAGES_PATH / f'{n_grams}_performance_per_n_concepts.eps'), format='eps')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_grams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}